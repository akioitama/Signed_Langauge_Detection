# Signed_Langauge_Detection_(Real-Time)

This project implements a real-time sign language detection system using YOLO (You Only Look Once) for hand gesture recognition. It captures live video from your laptop camera and translates hand gestures into corresponding text or commands.

Key Features

Real-Time Detection: Uses your laptop camera to detect hand gestures live.

YOLO-Based Model: Trains a YOLO model on a dataset of sign language gestures for accurate detection.

Gesture-to-Text Translation: Converts recognized gestures into text output in real-time.

Preprocessing & Annotation: Supports training with annotated images for improved model performance.

Visualization: Highlights detected hand gestures with bounding boxes on live video feed.

Tech Stack

Python, OpenCV, NumPy, YOLO (YOLOv5/YOLOv8), PyTorch

Use Cases

Enables communication for hearing-impaired individuals.

Real-time gesture-based interaction and control applications.

Educational tools for learning and practicing sign language.
